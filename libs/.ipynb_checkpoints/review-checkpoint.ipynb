{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load ('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "### Every Review will be parsed as per this Review class            ###\n",
    "#######################################################################\n",
    "\n",
    "class Review:\n",
    "    def __init__ (self):\n",
    "        self.abbreviations = set ( \"i.e. e.g. am mr mrs dr. prof. kg lbs cm in m mm ft\".split() )\n",
    "        self.stop_words = nlp.Defaults.stop_words\n",
    "        if 'not' in self.stop_words:\n",
    "            self.stop_words.remove ('not')\n",
    "        # Removing adjectives fro\n",
    "        self.stop_words = set (filter (lambda x: nlp(x)[0].pos != 84, self.stop_words))\n",
    "        # This feature set would contain adjectives and verbs encountered throughout, the Reviews dataset\n",
    "        self.features = set()\n",
    "        \n",
    "    def remove_urls_hyperlinks (self, review):\n",
    "        pattern = r\"\\s*(http[s]?:[/]{2}www[.])?([a-z0-9]+)[.]([a-z]{3})([/][A-Za-z0-9?='.]*)*\\s*\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Pattern matched below urls and hyperlinks, i guess it works for now\n",
    "            http://www.google.com \n",
    "            https://www.google.com\n",
    "            www.google.com\n",
    "            google.com \n",
    "            zomato.com\n",
    "            zomato.com/\n",
    "            zomato.com/items/this?dir=hello\n",
    "            https://www.zomato.com/places/NewDelhi/bbqnation/12.html\n",
    "        \"\"\"\n",
    "        return re.sub (pattern, '', review)\n",
    "        \n",
    "        \n",
    "    def token_merge (self, doc):\n",
    "        # patterns for can't, didn't shouldn't wouldn't wasn't\n",
    "        verb_patterns = [\n",
    "            [{\"TEXT\": {\"REGEX\": r\"ca|did|should|would|was\"}}, {\"LOWER\": \"n't\"}],\n",
    "        ]\n",
    "        matcher = Matcher (nlp.vocab)\n",
    "        matcher.add ('neg_verbs', None, *verb_patterns)\n",
    "        for _,start,end in matcher (doc):\n",
    "            span = doc[start: end]\n",
    "            span.merge()\n",
    "        \n",
    "        return doc\n",
    "    \n",
    "    def to_string (self, doc):\n",
    "        \"\"\"\n",
    "            param:\n",
    "                doc -> an nlp object ie. object of spacy.load() instance\n",
    "        \"\"\"\n",
    "        if isinstance (doc, str):\n",
    "            # Means, function got a string instead of a doc\n",
    "            doc = nlp (doc)\n",
    "        # doc.ents -> returns a tuple of spacy spans\n",
    "        # spans -> returns tokens\n",
    "        ne_indices = set()\n",
    "        for span in doc.ents:\n",
    "            for token in span:\n",
    "                ne_indices.add (token.i)\n",
    "        msg = []\n",
    "        for token in doc:\n",
    "            if token.text.lower() in self.stop_words:\n",
    "                continue\n",
    "            if token.i not in ne_indices:\n",
    "                msg.append (str(token.text).lower())\n",
    "            else:\n",
    "                msg.append (token.text)\n",
    "        return ' '.join(msg)\n",
    "                \n",
    "        \n",
    "    def split_into_sentences (self, review):\n",
    "        \"\"\"\n",
    "            While Splitting a big chunk of review, we need to take care of a few things first\n",
    "            1. split on . (fullstops)\n",
    "            2. Don't split if the (.) is a part of any standard abbreviation\n",
    "            3. Split in case you found a conjunction joining the words, cause most of the time\n",
    "               that sentence will convey 2 different sentiments, we don't want that for now.\n",
    "            4. Drop Punctuations\n",
    "        \"\"\"\n",
    "        doc = nlp (review)\n",
    "        doc = self.token_merge (doc)\n",
    "        start = 0\n",
    "        splits = []\n",
    "        for token in doc:\n",
    "            # print (token, end=' # ')\n",
    "            ## Step 1:\n",
    "            if token.text.strip() == '.':\n",
    "                # Step 2: Check for previous token being an Abbreviation\n",
    "                if (token.i-1) >= 0 and doc[token.i - 1].text.lower() in self.abbreviations:\n",
    "                    pass\n",
    "                else:\n",
    "                    splits.append (doc[start: token.i])\n",
    "                    start = token.i + 1\n",
    "            # Step 3: Splitting on Conjunctions or ADP\n",
    "            elif token.pos == 89:\n",
    "                splits.append (doc[start: token.i])\n",
    "                start = token.i + 1\n",
    "                \n",
    "            #### Also, we now need to build Feature List of Adjectives and Verbs and Adverbs, so\n",
    "            ## 84: Adjectives | 100: verbs\n",
    "            if (token.pos == 84 or token.pos == 100 or token.pos == 86) and token.text.lower() not in nlp.Defaults.stop_words:\n",
    "                self.features.add (token.text.lower())\n",
    "                \n",
    "        if len (doc[start: ]) > 0:\n",
    "            splits.append (doc[start: ])\n",
    "        return splits\n",
    "    \n",
    "    def pre_process (self, msg):\n",
    "        msg = self.remove_urls_hyperlinks (msg)\n",
    "        sentences = self.split_into_sentences (msg)\n",
    "        # return self.split_into_sentences (msg)\n",
    "        return [self.to_string(span) for span in sentences]\n",
    "    \n",
    "    # For use in feature selection\n",
    "    def pos (self, span):\n",
    "        doc = nlp(span)\n",
    "        return set([t.pos for t in doc])\n",
    "        # return (token.pos, token.pos_, spacy.explain (token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking handling of NER\n",
      "mr. Narendra Modi prime minister India .\n",
      "food 92 NOUN\n",
      "not 86 ADV\n",
      "tasty 84 ADJ\n",
      "did 100 VERB\n",
      "n't 86 ADV\n",
      "taste 100 VERB\n",
      "good 84 ADJ\n",
      "Crust 96 PROPN NNP noun, proper singular\n",
      "was 100 VERB VBD verb, past tense\n",
      "not 86 ADV RB adverb\n",
      "good 84 ADJ JJ adjective\n",
      "{'taste', \"didn't\", 'good', 'not', 'tasty'}\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    r = Review ()\n",
    "    \n",
    "    print (\"Checking handling of NER\")\n",
    "    print (r.to_string (\"Mr. Narendra Modi is the Prime minister of India.\"))\n",
    "    \n",
    "    msg = \"Food was not tasty. It didn't taste good.\"\n",
    "    for t in nlp(\" \".join(r.pre_process(msg))):\n",
    "        print (t.text, t.pos, t.pos_)\n",
    "        \n",
    "    for span in r.split_into_sentences (\"Crust was not good.\"):\n",
    "        for t in span:\n",
    "            print (t, t.pos, t.pos_, t.tag_, spacy.explain (t.tag_))\n",
    "            \n",
    "    print (r.features)\n",
    "    print ('not' in nlp.Defaults.stop_words)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
