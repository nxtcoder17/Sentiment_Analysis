{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pickle\n",
    "import string\n",
    "import re\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment:\n",
    "    def __init__ (self):\n",
    "        self.nlp = spacy.load (\"en_core_web_sm\")\n",
    "        self.neg_words = pickle.load (open(\"../neg_words.pkl\", \"rb\"))\n",
    "        self.stop_words = spacy.lang.en.STOP_WORDS\n",
    "    \n",
    "        # By default, not is a stopword\n",
    "        if 'not' in self.stop_words:\n",
    "            self.stop_words.remove ('not')\n",
    "        \n",
    "        self.stop_words.update (string.punctuation)\n",
    "        self.split_pattern = re.compile (r\"(\\s|-)\")\n",
    "        \n",
    "    def vectorizer (self, X):\n",
    "        vect = TfidfVectorizer ()\n",
    "        return vect.fit_transform (X)\n",
    "    \n",
    "    def train_model (self, X, y):\n",
    "        #gaussian_model = GaussianNB()\n",
    "        multinomial_model = MultinomialNB()\n",
    "        \n",
    "        self.models = [multinomial_model]\n",
    "        \n",
    "        X = self.vectorizer (X)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split (X, y, test_size=.1, random_state=7)\n",
    "        \n",
    "        for model in self.models:\n",
    "            model.fit (X_train, y_train)\n",
    "            predicted = model.predict (X_test)\n",
    "            print (f\"\\t Confusion Matrix: {model}\")\n",
    "            print (confusion_matrix (y_test, predicted))\n",
    "            \n",
    "            print (f\"\\t Classification Matrix: {model}\")\n",
    "            print (classification_report (y_test, predicted))\n",
    "            \n",
    "            print (\"------------ Over -----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    def __init__ (self, reviews):\n",
    "        self.nlp = spacy.load ('en_core_web_sm')\n",
    "        self.neg_words = pickle.load (open (\"neg_words.pkl\", \"rb\"))\n",
    "        self.neg_words.add ('not')\n",
    "        \n",
    "        self.split_pattern = re.compile (r\"(\\s|-)\")\n",
    "        self.reviews = [self.nlp(x.lower()) for x in reviews]\n",
    "        \n",
    "        self.stop_words = STOP_WORDS\n",
    "        self.stop_words.update (string.punctuation)\n",
    "        self.stop_words = set([x for x in self.stop_words if x not in self.neg_words])\n",
    "        \n",
    "        self.docs = []\n",
    "    \n",
    "    def drop_adverbs (self):\n",
    "        for index in range(len(self.splits)):\n",
    "            advbs = set()\n",
    "            for t in self.splits[index]:\n",
    "                if t.pos == 86 and t.text not in self.neg_words:\n",
    "                    advbs.add (t.i)\n",
    "            self.splits[index] = [x for x in self.splits[index] if x.i not in advbs]\n",
    "            \n",
    "    def drop_stopwords(self):\n",
    "        # Remove all stop words that are present in the neg_words, TILL NOW: only 'not'\n",
    "        for i in range (len (self.splits)):\n",
    "            stpwrds = set ()\n",
    "            for t in self.splits[i]:\n",
    "                if t.text.lower() in self.stop_words:\n",
    "                    stpwrds.add (t.i)\n",
    "            self.splits[i] = [x for x in self.splits[i] if x.i not in stpwrds]\n",
    "\n",
    "    def sentence_splitter (self):\n",
    "        self.splits = []\n",
    "        def splitter (sentence):\n",
    "            start = 0\n",
    "            counter = 0\n",
    "            for token in sentence:\n",
    "                if token.pos == 89 or token.text.strip() == ',':\n",
    "                    if counter > start:\n",
    "                        self.splits.append (sentence[start: counter])\n",
    "                    start = counter + 1\n",
    "                counter += 1\n",
    "            #print (sentence[start: ])\n",
    "            if len (sentence[start: counter]) > 0:\n",
    "                self.splits.append (sentence[start: counter])\n",
    "        \n",
    "        for doc in self.reviews:\n",
    "            for sent in doc.sents:\n",
    "                #print (\"Sentence: \", sent)\n",
    "                splitter (sent)\n",
    "        #return splits\n",
    "        \n",
    "    def feature_extraction (self):\n",
    "        # To be done when splits have SPAN Objects\n",
    "        # i.e. before any drop_*** methods()\n",
    "        self.features = []\n",
    "        for entry in self.splits:\n",
    "            # For every entry, we need to pick out a noun and an adjective\n",
    "            nouns=[]\n",
    "            adjs=[]\n",
    "            vbs = []\n",
    "            for t in entry:\n",
    "                if t.pos == 92 or t.pos == 96:\n",
    "                    nouns.append (t)\n",
    "                elif t.pos == 84:\n",
    "                    adjs.append (t)\n",
    "                elif t.pos == 100:\n",
    "                    vbs.append (t)\n",
    "            if len (adjs) == 0:\n",
    "                adjs = vbs\n",
    "            self.features.append ( \n",
    "                                        ( ', '.join(map(str, nouns)), ', '.join(map(str, adjs)) ),\n",
    "                                  )\n",
    "        \n",
    "    def prepare(self):\n",
    "        self.sentence_splitter()\n",
    "        self.feature_extraction()\n",
    "        self.drop_adverbs()\n",
    "        \n",
    "    def pprint (self):\n",
    "        counter = 0\n",
    "        for split in sa.splits:\n",
    "            sent = ' '.join (map(str, split))\n",
    "            print (sent)\n",
    "            scores = sia.polarity_scores (sent)\n",
    "            print (scores)\n",
    "            print (f\"ENTITY: {sa.features[counter][0]:<10} \\tFEATURES: {sa.features[counter][1]:<10}\", end=\"\\t\")\n",
    "            if scores['compound'] > 0:\n",
    "                print (\"POSITIVE\")\n",
    "            elif scores['compound'] == 0:\n",
    "                print (\"NEUTRAL\")\n",
    "            else:\n",
    "                print (\"NEGATIVE\");\n",
    "            print()\n",
    "            counter += 1\n",
    "        print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv (\"../Resturant_Reviews/Restaurant_Reviews.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:]['Review']\n",
    "y = df.iloc[:]['Liked']http://codeforces.com/problemset/problem/158/B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             Wow... Loved this place.\n",
       "1                                   Crust is not good.\n",
       "2            Not tasty and the texture was just nasty.\n",
       "3    Stopped by during the late May bank holiday of...\n",
       "4    The selection on the menu was great and so wer...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    0\n",
       "3    1\n",
       "4    1\n",
       "Name: Liked, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = Sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Confusion Matrix: MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "[[42 16]\n",
      " [10 32]]\n",
      "\t Classification Matrix: MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.72      0.76        58\n",
      "           1       0.67      0.76      0.71        42\n",
      "\n",
      "   micro avg       0.74      0.74      0.74       100\n",
      "   macro avg       0.74      0.74      0.74       100\n",
      "weighted avg       0.75      0.74      0.74       100\n",
      "\n",
      "------------ Over -----------------\n"
     ]
    }
   ],
   "source": [
    "sa.train_model (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load (\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food 92 NOUN NN\n",
      "was 100 VERB VBD\n",
      "amazing 84 ADJ JJ\n",
      "here 86 ADV RB\n",
      ". 97 PUNCT .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp (\"Food was amazing here.\")\n",
    "for token in doc:\n",
    "    print (token, token.pos, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
